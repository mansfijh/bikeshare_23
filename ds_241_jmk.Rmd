---
title: "Final Project"
output: html_notebook
authors: "Joshua Allessio, Kashyap Vallur and Marco Camalich"
---

## Problem Statement
Washington DC's public bikeshare dataset is a classic data science project. Much can be gleaned from this data. In this report, we set out to discover which of the bike stations are particularly efficient or inefficient. The goal is to create a visualization of the net flow of bikes for each station over time. 

If the net flow for a bike station often stays close to 0, we may assume that that station is in an optimal position and hosts enough bicycles. Furthermore, high turnover of bikes can imply that a bike station is in an optimal position, while low turnover of bikes may imply that that bike station is not placed in an optimal location. 

Another pattern that may arise could be a bike station that has a strong negative netflow early in the day, followed by a stagnant period of netflow later in the day. This would imply that the bike station has quickly become vacant - now offering no bicycles for potential riders. Potential solution for this particular problem might be increasing the size of that station, creating a new one nearby, or making a larger change to the infrastructure of the bikeshare. 


```{r}
required_packages <- c("tidyverse", "janitor", "here", "openmeteo", "lubridate", "forcats", "dplyr")

for (packages in required_packages) {
  if (!require(packages, character.only = TRUE)) {
    install.packages(packages, dependencies = TRUE)
    library(packages, character.only = TRUE)
  }
}

if (all(sapply(required_packages, requireNamespace, quietly = TRUE))) {
  cat("All required packages are loaded.\n")
} else {
  warning("Some packages failed to load.")
}

```

## Create a function to filter useless data from the data set
This function takes the DC bikeshare data sets and removes columns with NA values, as well as rides which lasted
less than 30 seconds or more than 13 hours. 
```{r}
filter_bikeshare <- function(df) {
  df |> filter(
    !is.na(start_station_name) & !is.na(start_station_id) & start_station_name != "" & start_station_id != "" &
    !is.na(end_station_name) & !is.na(end_station_id) & end_station_name != "" & end_station_id != "" &
    !is.na(start_lat) & !is.na(start_lng) & !is.na(end_lng)    
  )
  df <- df |> 
    mutate(duration = as.numeric(difftime(ended_at, started_at))) |>  
    filter(duration > 30) |>  # Longer than 30 seconds
    filter (duration < 46800) # Less than 13 hours
return(df)
}
```


## Import the data
```{r}
df_01=read_csv(here("data_raw", "202301-capitalbikeshare-tripdata.csv"))
df_02=read_csv(here("data_raw", "202302-capitalbikeshare-tripdata.csv"))
df_03=read_csv(here("data_raw", "202303-capitalbikeshare-tripdata.csv"))
df_04=read_csv(here("data_raw", "202304-capitalbikeshare-tripdata.csv"))
df_05=read_csv(here("data_raw", "202305-capitalbikeshare-tripdata.csv"))
df_06=read_csv(here("data_raw", "202306-capitalbikeshare-tripdata.csv"))
df_07=read_csv(here("data_raw", "202307-capitalbikeshare-tripdata.csv"))
df_08=read_csv(here("data_raw", "202308-capitalbikeshare-tripdata.csv"))
df_09=read_csv(here("data_raw", "202309-capitalbikeshare-tripdata.csv"))
df_10=read_csv(here("data_raw", "202310-capitalbikeshare-tripdata.csv"))
```

## Tidy the Data
```{r}
df_01 <- filter_bikeshare(df_01)
df_02 <- filter_bikeshare(df_02)
df_03 <- filter_bikeshare(df_03)
df_04 <- filter_bikeshare(df_04)
df_05 <- filter_bikeshare(df_05)
df_06 <- filter_bikeshare(df_06)
df_07 <- filter_bikeshare(df_07)
df_08 <- filter_bikeshare(df_08)
df_09 <- filter_bikeshare(df_09)
df_10 <- filter_bikeshare(df_10)
```

## Aggregate the Data
Aggregate the data into one data frame and begin with a simple visualization
```{r}
all_data <- bind_rows(
  df_01, df_02, df_03, df_04, df_05, df_06, df_07, df_08, df_09, df_10,
  .id = "month"
)

all_data$started_at <- as.POSIXct(all_data$started_at, format = "%Y-%m-%d %H:%M:%S")
all_data$month <- format(all_data$started_at, "%Y-%m")

ggplot(all_data, aes(x = month)) +
  geom_bar() +
  labs(title = "Monthly Ride Counts",
       x = "Month",
       y = "Ride Counts") +
  theme_minimal()
```
```{r}
unique_station_count <- all_data %>%
  filter(!is.na(start_station_name) & !is.na(start_station_id) & start_station_name != "" & start_station_id != "") %>%
  summarise(unique_stations = n_distinct(start_station_name))

cat("The total amount of unique stations is:", unique_station_count$unique_stations, "\n")
```

```{r}
# Combine start and end stations into one column
all_stations <- bind_rows(
  select(all_data, station = start_station_name),
  select(all_data, station = end_station_name)
)

# Get the top 5 stations (excluding NA)
top_stations <- all_stations %>%
  filter(!is.na(station)) %>%
  count(station, sort = TRUE) %>%
  head(5)

# Print the top 5 stations
print(top_stations)
```
```{r}
# Graph the top 5 stations
```

```{r}
# Get the top 5 stations (excluding NA)
worst_stations <- all_stations %>%
  filter(!is.na(station)) %>%
  count(station, sort = TRUE) %>%
  filter(n < 50) %>%

# Print the top 5 stations
print(worst_stations)
```
```{r}
# Get the top 5 stations with n >= 100 (excluding NA)
bad_stations <- all_stations %>%
  filter(!is.na(station)) %>%
  count(station, sort = TRUE) %>%
  filter(n >= 50) %>%
  tail(5)

# Print the top 5 stations
print(bad_stations)
```
```{r}
# Graph the worst 5 stations
```

```{r}
ggplot(all_data, aes(x = month, fill = rideable_type)) +
  geom_bar(position = "fill") +
  labs(title = "Popular Rideable Types by Month",
       x = "Month",
       y = "Proportion") +
  theme_minimal() +
  scale_y_continuous(labels = scales::percent_format(scale = 100))
```


